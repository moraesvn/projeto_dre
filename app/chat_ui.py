from __future__ import annotations
from typing import Callable, Any
import streamlit as st

# Tipo de callback opcional: recebe (pergunta:str, filtros:Any) -> str (resposta do agente)
OnAsk = Callable[[str, Any], str]


def render_chat(area_key: str, filtros: Any, on_ask: OnAsk | None = None) -> None:
    """
    Renderiza um chat simples com hist√≥rico.
    - `area_key` diferencia o hist√≥rico por p√°gina (ex.: "financeiro").
    - `filtros` √© repassado ao agente/callback.
    - `on_ask` (opcional) permite injetar uma fun√ß√£o que chama o agente.
      Se n√£o for passado, tenta usar `ai.generate_insights` como fallback.
    """
    state_key = f"chat_{area_key}_messages"
    if state_key not in st.session_state:
        st.session_state[state_key] = []

    st.subheader("üí¨ Chat com o CFO (IA)")

    # Bot√µes utilit√°rios
    col_a, col_b = st.columns([1, 6])
    with col_a:
        if st.button("Limpar conversa", use_container_width=True):
            st.session_state[state_key] = []
            st.experimental_rerun()

    # Exibe hist√≥rico
    for msg in st.session_state[state_key]:
        with st.chat_message(msg.get("role", "assistant")):
            st.markdown(msg.get("content", ""))

    # Entrada de chat
    prompt = st.chat_input("Pergunte ao CFO sobre os dados deste per√≠odo‚Ä¶")
    if not prompt:
        return

    # Registra e mostra a mensagem do usu√°rio
    st.session_state[state_key].append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Resposta do agente
    try:
        if on_ask is not None:
            resposta = on_ask(prompt, filtros)
    except Exception as e:  # noqa: BLE001
        resposta = f"‚ö†Ô∏è IA indispon√≠vel: {e}"

    st.session_state[state_key].append({"role": "assistant", "content": resposta})
    with st.chat_message("assistant"):
        st.markdown(resposta)
